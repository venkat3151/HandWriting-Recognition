{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import csv \n",
    "import operator\n",
    "from random import sample\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingPercent = 80 #Percentage of data considered for training the model\n",
    "ValidationPercent = 10 #Percentage of data considered for validation\n",
    "TestPercent = 10 #Percentage of data considered for testing\n",
    "C_Lambda = 0.03 #term used for regularization in order to prevent overfitting\n",
    "\n",
    "def generateRawData(filename) : \n",
    "    raw_data = open(filename, 'r')\n",
    "    reader = csv.reader(raw_data, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "    next(reader)\n",
    "    x = list(reader)\n",
    "    data = np.array(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1026, 11)\n",
      "(293032, 3)\n",
      "(791, 3)\n",
      "(14072, 513)\n",
      "(762557, 3)\n",
      "(71531, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reading the matrix and random shuffling of the datasets\n",
    "human_observed_dataset = generateRawData('HumanObserved-Features-Data.csv')\n",
    "print(type(human_observed_dataset))\n",
    "np.array(np.random.shuffle(human_observed_dataset))\n",
    "print(type(human_observed_dataset))\n",
    "diffn_pairs_data = generateRawData('diffn_pairs.csv')\n",
    "np.array(np.random.shuffle(diffn_pairs_data))\n",
    "gsc_diffn_pair = generateRawData('GSC-Features-Data/diffn_pairs.csv')\n",
    "np.array(np.random.shuffle(gsc_diffn_pair))\n",
    "same_pairs_data = generateRawData('same_pairs.csv')\n",
    "np.array(np.random.shuffle(same_pairs_data))\n",
    "gsc_same_pair = generateRawData('GSC-Features-Data/same_pairs.csv')\n",
    "np.array(np.random.shuffle(gsc_same_pair))\n",
    "gsc_dataset = generateRawData('GSC-Features-Data/GSC-Features.csv')\n",
    "np.array(np.random.shuffle(gsc_dataset))\n",
    "\n",
    "print(human_observed_dataset.shape)\n",
    "print(diffn_pairs_data.shape)\n",
    "print(same_pairs_data.shape)\n",
    "print(gsc_dataset.shape)\n",
    "print(gsc_diffn_pair.shape)\n",
    "print(gsc_same_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSC 14072\n",
      "GSC length 512\n"
     ]
    }
   ],
   "source": [
    "img_id = []\n",
    "gsc_img_id = []\n",
    "feature_vector = []\n",
    "same_pairs_target = []\n",
    "diff_pairs_target = []\n",
    "total_target = []\n",
    "gsc_feature_vector = []\n",
    "########### Human Observed #################\n",
    "feature_vector = human_observed_dataset[:,2: ]\n",
    "feature_vector = [[int(y) for y in x] for x in feature_vector]\n",
    "############## GSC ########################\n",
    "gsc_feature_vector = gsc_dataset[:,1: ]\n",
    "gsc_feature_vector = [[int(y) for y in x] for x in gsc_feature_vector]\n",
    "print('GSC ' + str(len(gsc_feature_vector)))\n",
    "print('GSC length ' + str(len(gsc_feature_vector[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same pair target shape is : 791\n",
      "Diff pair target shape is : 791\n",
      "Total target shape is : 1582\n",
      "Total target type is : <class 'numpy.ndarray'>\n",
      "[1 1 1 ... 0 0 0]\n",
      "GSC Same pair target shape is : 2000\n",
      "GSC Diff pair target shape is : 2000\n",
      "GSC Total target shape is : 4000\n",
      "GSC Total target type is : <class 'numpy.ndarray'>\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "########### Human Observed #################\n",
    "same_pairs_target = same_pairs_data[:,-1]\n",
    "same_pairs_target = same_pairs_target.astype(np.int_)\n",
    "print('Same pair target shape is : '+ str(len(same_pairs_target)))\n",
    "\n",
    "diff_pairs_target = diffn_pairs_data[0:791,-1]\n",
    "diff_pairs_target = diff_pairs_target.astype(np.int_)\n",
    "print('Diff pair target shape is : '+ str(len(diff_pairs_target)))\n",
    "\n",
    "total_target = np.concatenate((same_pairs_target, diff_pairs_target))\n",
    "print('Total target shape is : '+ str(len(total_target)))\n",
    "print('Total target type is : '+ str(type(total_target)))\n",
    "print(total_target)\n",
    "############## GSC ########################\n",
    "gsc_same_pairs_target = gsc_same_pair[0:2000,-1]\n",
    "gsc_same_pairs_target = gsc_same_pairs_target.astype(np.int_)\n",
    "print('GSC Same pair target shape is : '+ str(len(gsc_same_pairs_target)))\n",
    "\n",
    "gsc_diff_pairs_target = gsc_diffn_pair[0:2000,-1]\n",
    "gsc_diff_pairs_target = gsc_diff_pairs_target.astype(np.int_)\n",
    "print('GSC Diff pair target shape is : '+ str(len(gsc_diff_pairs_target)))\n",
    "\n",
    "gsc_total_target = np.concatenate((gsc_same_pairs_target, gsc_diff_pairs_target))\n",
    "print('GSC Total target shape is : '+ str(len(gsc_total_target)))\n",
    "print('GSC Total target type is : '+ str(type(gsc_total_target)))\n",
    "print(gsc_total_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14072\n"
     ]
    }
   ],
   "source": [
    "img_id = []\n",
    "gsc_img_id = []\n",
    "########### Human Observed #################\n",
    "with open('HumanObserved-Features-Data.csv', 'r') as rf:\n",
    "    read = csv.reader(rf)\n",
    "    next(read)\n",
    "    for i in read:\n",
    "        img_id.extend([i[1]])\n",
    "\n",
    "############## GSC ########################\n",
    "with open('GSC-Features-Data/GSC-Features.csv', 'r') as grf:\n",
    "    gread = csv.reader(grf)\n",
    "    next(gread)\n",
    "    for j in gread:\n",
    "        gsc_img_id.extend([j[0]])\n",
    "    print(str(len(gsc_img_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Same pair concatenation shape is : 791\n",
      "Same pair Subtraction shape is : 791\n",
      "<class 'list'>\n",
      "GSC Same pair concatenation shape is : 2000\n",
      "GSC Same pair concatenation shape is : 1024\n",
      "GSC Same pair Subtraction shape is : 2000\n"
     ]
    }
   ],
   "source": [
    "########### Human Observed #################    \n",
    "a=[]\n",
    "b=[]\n",
    "same_pair_concatenation=[]\n",
    "same_pair_subtraction = []\n",
    "diff_pair_concateantion=[]\n",
    "diff_pair_subtraction = []\n",
    "total_human_concat_data =[]\n",
    "total_human_subtract_data=[]\n",
    "with open('same_pairs.csv', 'r') as fi:\n",
    "    same_read = csv.reader(fi)\n",
    "    next(same_read)\n",
    "    for i in same_read:\n",
    "        a = feature_vector[img_id.index(i[0])]\n",
    "        b = feature_vector[img_id.index(i[1])]\n",
    "        same_pair_subtraction.append(list(map(operator.sub, a, b)))\n",
    "        same_pair_concatenation.append(a+b)   \n",
    "print(type(same_pair_concatenation))\n",
    "print('Same pair concatenation shape is : '+str(len(same_pair_concatenation)))\n",
    "print('Same pair Subtraction shape is : '+ str(len(same_pair_subtraction)))\n",
    "\n",
    "############## GSC ########################\n",
    "count = 0\n",
    "ga=[]\n",
    "gb=[]\n",
    "gsc_same_pair_concatenation=[]\n",
    "gsc_same_pair_subtraction = []\n",
    "gsc_total_human_concat_data =[]\n",
    "gsc_total_human_subtract_data=[]\n",
    "with open('GSC-Features-Data/same_pairs.csv', 'r') as gfi:\n",
    "    gsame_read = csv.reader(gfi)\n",
    "    next(gsame_read)\n",
    "    for i in gsame_read:\n",
    "        count+=1\n",
    "        ga = gsc_feature_vector[gsc_img_id.index(i[0])]\n",
    "        gb = gsc_feature_vector[gsc_img_id.index(i[1])]\n",
    "        gsc_same_pair_subtraction.append(list(map(operator.sub, ga, gb)))\n",
    "        gsc_same_pair_concatenation.append(ga+gb)\n",
    "        if count == 2000 :\n",
    "            break\n",
    "print(type(same_pair_concatenation))\n",
    "print('GSC Same pair concatenation shape is : '+str(len(gsc_same_pair_concatenation)))\n",
    "print('GSC Same pair concatenation shape is : '+str(len(gsc_same_pair_concatenation[0])))\n",
    "print('GSC Same pair Subtraction shape is : '+ str(len(gsc_same_pair_subtraction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Diff pair Subtraction shape is : 791\n",
      "Diff pair concatenation shape is : 791\n",
      "<class 'list'>\n",
      "GSC Diff pair Subtraction shape is : 2000\n",
      "GSC Diff pair concatenation shape is : 2000\n"
     ]
    }
   ],
   "source": [
    "gsc_diff_pair_concateantion=[]\n",
    "gsc_diff_pair_subtraction = []\n",
    "########### Human Observed #################  \n",
    "with open('diffn_pairs.csv', 'r') as fi:\n",
    "    diff_read = csv.reader(fi)\n",
    "    next(diff_read)\n",
    "    for j in diff_read:\n",
    "        a = feature_vector[img_id.index(j[0])]\n",
    "        b = feature_vector[img_id.index(j[1])]\n",
    "        diff_pair_concateantion.append(a+b)\n",
    "        diff_pair_subtraction.append(list(map(operator.sub, a, b)))\n",
    "diff_pair_concateantion = diff_pair_concateantion[:791]\n",
    "diff_pair_subtraction = diff_pair_subtraction[:791]\n",
    "print(type(diff_pair_concateantion))\n",
    "print('Diff pair Subtraction shape is : '+str(len(diff_pair_subtraction)))\n",
    "print('Diff pair concatenation shape is : '+str(len(diff_pair_concateantion)))\n",
    "\n",
    "############## GSC ########################\n",
    "count = 0\n",
    "with open('GSC-Features-Data/diffn_pairs.csv', 'r') as gfi:\n",
    "    gdiff_read = csv.reader(gfi)\n",
    "    next(gdiff_read)\n",
    "    for j in gdiff_read:\n",
    "        count+=1\n",
    "        ga = gsc_feature_vector[gsc_img_id.index(j[0])]\n",
    "        gb = gsc_feature_vector[gsc_img_id.index(j[1])]\n",
    "        gsc_diff_pair_concateantion.append(ga+gb)\n",
    "        gsc_diff_pair_subtraction.append(list(map(operator.sub, ga, gb)))\n",
    "        if count == 2000 :\n",
    "            break\n",
    "print(type(diff_pair_concateantion))\n",
    "print('GSC Diff pair Subtraction shape is : '+str(len(gsc_diff_pair_concateantion)))\n",
    "print('GSC Diff pair concatenation shape is : '+str(len(gsc_diff_pair_subtraction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO. of Rows in Total Human Concatenated Data : 18\n",
      "NO. of Columns in Total Human Concatenated Data : 1582\n",
      "NO. of Rows in Total Human Subtracted Data : 9\n",
      "NO. of Columns in Total Human Subtracted Data : 1582\n",
      "NO. of Rows in Total GSC Concatenated Data : 1024\n",
      "NO. of Columns in Total GSC Concatenated Data : 4000\n",
      "NO. of Rows in Total GSC Subtracted Data : 512\n",
      "NO. of Columns in Total GSC Subtracted Data : 4000\n"
     ]
    }
   ],
   "source": [
    "########### Human Observed #################  \n",
    "total_human_concat_data = same_pair_concatenation + diff_pair_concateantion\n",
    "total_human_concat_data = np.transpose(total_human_concat_data) \n",
    "print('NO. of Rows in Total Human Concatenated Data : ' + str(len(total_human_concat_data)))\n",
    "print('NO. of Columns in Total Human Concatenated Data : ' + str(len(total_human_concat_data[0])))\n",
    "total_human_subtract_data = same_pair_subtraction + diff_pair_subtraction\n",
    "total_human_subtract_data = np.transpose(total_human_subtract_data)\n",
    "print('NO. of Rows in Total Human Subtracted Data : ' + str(len(total_human_subtract_data)))\n",
    "print('NO. of Columns in Total Human Subtracted Data : ' + str(len(total_human_subtract_data[0])))\n",
    "\n",
    "############## GSC ########################\n",
    "gsc_total_human_concat_data =[]\n",
    "gsc_total_human_subtract_data=[]\n",
    "\n",
    "gsc_total_human_concat_data = gsc_same_pair_concatenation + gsc_diff_pair_concateantion\n",
    "gsc_total_human_concat_data = np.transpose(gsc_total_human_concat_data) \n",
    "print('NO. of Rows in Total GSC Concatenated Data : ' + str(len(gsc_total_human_concat_data)))\n",
    "print('NO. of Columns in Total GSC Concatenated Data : ' + str(len(gsc_total_human_concat_data[0])))\n",
    "\n",
    "gsc_total_human_subtract_data = gsc_same_pair_subtraction + gsc_diff_pair_subtraction\n",
    "gsc_total_human_subtract_data = np.transpose(gsc_total_human_subtract_data)\n",
    "print('NO. of Rows in Total GSC Subtracted Data : ' + str(len(gsc_total_human_subtract_data)))\n",
    "print('NO. of Columns in Total GSC Subtracted Data : ' + str(len(gsc_total_human_subtract_data[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare trainig data\n",
    "# This is taking 80% of the target values from the entire target \n",
    "def GenerateTrainingTarget (rawTraining,TrainingPercent):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t  \n",
    "# This method is used to specify and seperate 80% of the data is for training data\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1266,)\n",
      "(3200,)\n",
      "(18, 1266)\n",
      "(9, 1266)\n",
      "(1024, 3200)\n",
      "(512, 3200)\n"
     ]
    }
   ],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(total_target,TrainingPercent))\n",
    "GSCTrainingTarget = np.array(GenerateTrainingTarget(gsc_total_target,TrainingPercent))\n",
    "TrainingDataForConcatenationHuman   = GenerateTrainingDataMatrix(total_human_concat_data,TrainingPercent)\n",
    "TrainingDataForConcatenationGSC   = GenerateTrainingDataMatrix(gsc_total_human_concat_data,TrainingPercent)\n",
    "TrainingDataForSubtractionHuman   = GenerateTrainingDataMatrix(total_human_subtract_data,TrainingPercent)\n",
    "TrainingDataForSubtractionGSC   = GenerateTrainingDataMatrix(gsc_total_human_subtract_data,TrainingPercent)\n",
    "print(TrainingTarget.shape)\n",
    "print(GSCTrainingTarget.shape)\n",
    "print(TrainingDataForConcatenationHuman.shape)\n",
    "print(TrainingDataForSubtractionHuman.shape)\n",
    "print(TrainingDataForConcatenationGSC.shape)\n",
    "print(TrainingDataForSubtractionGSC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is used to specify and seperate 10% of the data is for validation data.\n",
    "# The same method is called for testing data as well because it is the same 10% again.\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "# This method is for the file QueryLevelNormt.csv\n",
    "# prepares the target values for validation dataset\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158,)\n",
      "(399,)\n",
      "(18, 158)\n",
      "(9, 158)\n",
      "(1024, 399)\n",
      "(512, 399)\n"
     ]
    }
   ],
   "source": [
    "# genertae validation data and its target values\n",
    "# genertae validation data and its target values\n",
    "ValDataAct = np.array(GenerateValTargetVector(total_target,ValidationPercent, (len(TrainingTarget))))\n",
    "GSCVAlDataAct = np.array(GenerateValTargetVector(gsc_total_target,ValidationPercent, (len(GSCTrainingTarget))))\n",
    "ValDataForHumanConcatenation    = GenerateValData(total_human_concat_data,ValidationPercent, (len(TrainingTarget)))\n",
    "ValDataForGSCConcatenation    = GenerateValData(gsc_total_human_concat_data,ValidationPercent, (len(GSCTrainingTarget)))\n",
    "ValDataForSubtractionHuman   = GenerateValData(total_human_subtract_data,ValidationPercent, (len(TrainingTarget)))\n",
    "ValDataForSubtractionGSC   = GenerateValData(gsc_total_human_subtract_data,ValidationPercent, (len(GSCTrainingTarget)))\n",
    "print(ValDataAct.shape)\n",
    "print(GSCVAlDataAct.shape)\n",
    "print(ValDataForHumanConcatenation.shape)\n",
    "print(ValDataForSubtractionHuman.shape)\n",
    "print(ValDataForGSCConcatenation.shape)\n",
    "print(ValDataForSubtractionGSC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157,)\n",
      "(399,)\n",
      "(18, 157)\n",
      "(9, 157)\n",
      "(1024, 399)\n",
      "(512, 399)\n"
     ]
    }
   ],
   "source": [
    "# genertae testing data and its target values\n",
    "TestDataAct = np.array(GenerateValTargetVector(total_target,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "GSCTestDataAct = np.array(GenerateValTargetVector(gsc_total_target,TestPercent, (len(GSCTrainingTarget)+len(GSCVAlDataAct))))\n",
    "TestDataForConcatenationHuman = GenerateValData(total_human_concat_data,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n",
    "TestDataForGSCConcatenation    = GenerateValData(gsc_total_human_concat_data,TestPercent, (len(GSCTrainingTarget)+len(GSCVAlDataAct)))\n",
    "TestDataForSubtractionHuman   = GenerateValData(total_human_subtract_data,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n",
    "TestDataForSubtractionGSC   = GenerateValData(gsc_total_human_subtract_data,TestPercent, (len(GSCTrainingTarget)+len(GSCVAlDataAct)))\n",
    "print(TestDataAct.shape)\n",
    "print(GSCTestDataAct.shape)\n",
    "print(TestDataForConcatenationHuman.shape)\n",
    "print(TestDataForSubtractionHuman.shape)\n",
    "print(TestDataForGSCConcatenation.shape)\n",
    "print(TestDataForSubtractionGSC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to generate the bigsigma matrix which is used in the normal distribution formula.\n",
    "# Big sigma is the variance matrix. \n",
    "# We assume all varainces are equal for a particular feature.\n",
    "# Hence the variance matrix will be of the shape 41 * 41.\n",
    "# it is not 46 * 46 because 5 features are being deleted.\n",
    "# we ll have elements only in the diagnol of the matrix.\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))\n",
    "    varVect     = []\n",
    "    for i in range(0,(DataT.shape[1])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen) ):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]+0.2\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "# The gaussian distribution function is 0.5(X-Mu)* BigSigInv* (x-Mu)transpose.\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow) # This subtracts X- Mu and stores in R\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  # Multiplies R transpose and BigSigInv and stores in T\n",
    "    L = np.dot(R,T) # Multiplies T and R and returns L\n",
    "    return L\n",
    "# This functions is the gaussian distribution formula.\n",
    "# This is used to return each PHI and not the PHI matrix.\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "# This method is used to get the Phi matrix which is the design matrix.\n",
    "# Phi matrix is the result of the radial basis function.\n",
    "# We are using 10 radial basis function and hence dimension of phi matrix will be 10 * number of data\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01 ))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma) # We calculated BigSigma only. But we needed Inverse of Big Sigma acc. to the formula\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)#Every PHI calculated is stored in a Matrix called PHI\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "# Lamba is a regularization term used to prevent overfitting\n",
    "# This method is used to get the closed form W* \n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    print (INTER.shape)\n",
    "    print (T.shape)\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "# This is for the regression formula y = phi *w transpose\n",
    "# returns our predicted value\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "# This is used to find our accuracy and Erms based on target and the predicted value.\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def skeletonForLinear(TrainingDataForConcatenationHuman, total_human_concat_data, TrainingPercent,TrainingTarget,C_Lambda,TestDataForConcatenationHuman,ValDataForHumanConcatenation):\n",
    "    kmeans = KMeans(n_clusters=20, random_state=0).fit(np.transpose(TrainingDataForConcatenationHuman))\n",
    "    # We have to select one representative from each cluster.\n",
    "    # we are choosing the center of each cluster to be the representative.\n",
    "    # This is also known as the feature points.\n",
    "    Mu = kmeans.cluster_centers_\n",
    "    print(Mu.shape)\n",
    "    print(total_human_concat_data.shape)\n",
    "    BigSigma     = GenerateBigSigma(total_human_concat_data, Mu, TrainingPercent)\n",
    "    TRAINING_PHI = GetPhiMatrix(total_human_concat_data, Mu, BigSigma, TrainingPercent)\n",
    "    W            = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget, (C_Lambda)) \n",
    "    #Before Calculating Validation/Testing error, we need to create validation/Testing Design matrix.\n",
    "    # We cannot use the same Training Data design matrix.\n",
    "    TEST_PHI     = GetPhiMatrix(TestDataForConcatenationHuman, Mu, BigSigma, 100) \n",
    "    VAL_PHI      = GetPhiMatrix(ValDataForHumanConcatenation, Mu, BigSigma, 100)\n",
    "    print(BigSigma.shape)\n",
    "    print(TRAINING_PHI.shape)\n",
    "    print(W.shape)\n",
    "    print(VAL_PHI.shape)\n",
    "    print(TEST_PHI.shape)\n",
    "    return TRAINING_PHI,W,TEST_PHI,VAL_PHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 18)\n",
      "(18, 1582)\n",
      "(20, 1266)\n",
      "(1266,)\n",
      "(18, 18)\n",
      "(1266, 20)\n",
      "(20,)\n",
      "(158, 20)\n",
      "(157, 20)\n",
      "(20, 9)\n",
      "(9, 1582)\n",
      "(20, 1266)\n",
      "(1266,)\n",
      "(9, 9)\n",
      "(1266, 20)\n",
      "(20,)\n",
      "(158, 20)\n",
      "(157, 20)\n"
     ]
    }
   ],
   "source": [
    "TRAINING_PHI,W,TEST_PHI,VAL_PHI = skeletonForLinear(TrainingDataForConcatenationHuman, total_human_concat_data, TrainingPercent,TrainingTarget,C_Lambda,TestDataForConcatenationHuman,ValDataForHumanConcatenation)\n",
    "SUB_TRAINING_PHI,SUB_W,SUB_TEST_PHI,SUB_VAL_PHI = skeletonForLinear(TrainingDataForSubtractionHuman, total_human_subtract_data, TrainingPercent,TrainingTarget,C_Lambda,TestDataForSubtractionHuman,ValDataForSubtractionHuman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1024)\n",
      "(1024, 4000)\n",
      "(20, 3200)\n",
      "(3200,)\n",
      "(1024, 1024)\n",
      "(3200, 20)\n",
      "(20,)\n",
      "(399, 20)\n",
      "(399, 20)\n",
      "(20, 512)\n",
      "(512, 4000)\n",
      "(20, 3200)\n",
      "(3200,)\n",
      "(512, 512)\n",
      "(3200, 20)\n",
      "(20,)\n",
      "(399, 20)\n",
      "(399, 20)\n"
     ]
    }
   ],
   "source": [
    "GSC_TRAINING_PHI,GSC_W,GSC_TEST_PHI,GSC_VAL_PHI = skeletonForLinear(TrainingDataForConcatenationGSC, gsc_total_human_concat_data, TrainingPercent,GSCTrainingTarget,C_Lambda,TestDataForGSCConcatenation,ValDataForGSCConcatenation)\n",
    "GSC_SUB_TRAINING_PHI,GSC_SUB_W,GSC_SUB_TEST_PHI,GSC_SUB_VAL_PHI = skeletonForLinear(TrainingDataForSubtractionGSC, gsc_total_human_subtract_data, TrainingPercent,GSCTrainingTarget,C_Lambda,TestDataForSubtractionGSC,ValDataForSubtractionGSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeletonForSgd(TRAINING_PHI,W,TEST_PHI,VAL_PHI,ValDataAct,TestDataAct,TrainingTarget):\n",
    "    # The w* obtained from previous method is multiplied with 220 a scalar value\n",
    "    # This is because SGD  first takes a random initial value w(0)\n",
    "    W_Now        = np.dot(3, W)\n",
    "    # La is the lambda which as stated earlier is used as a regularizer in order to prevent overfitting\n",
    "    La           = 0.2 \n",
    "    learningRate = 0.01\n",
    "    # Learning rate in SGD must not be too high as it might never converge or if it is too low it will take lot of time to converge\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "\n",
    "    for i in range(0,200):\n",
    "        # Number of data considered in the entire dataset\n",
    "        # The next 4 steps are based on the formula where we calculate the weights based on previous iteration.\n",
    "        # They are used to adjust the weights based on previous value.\n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W\n",
    "        W_Now         = W_T_Next # W_Now will be the updated weight now\n",
    "\n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,TestDataAct)\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "    \n",
    "    return min(L_Erms_TR),min(L_Erms_Val),min(L_Erms_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gradient Descent Solution For Human Observed Data Concatenated--------------------\n",
      "Accuracy Training   = 0.7307\n",
      "Accuracy Validation = 0.76307\n",
      "Accuracy Testing    = 0.59972\n",
      "----------Gradient Descent Solution For Human Observed Data Subtracted--------------------\n",
      "Accuracy Training   = 0.77775\n",
      "Accuracy Validation = 0.8452\n",
      "Accuracy Testing    = 0.88312\n"
     ]
    }
   ],
   "source": [
    "L_Erm,L_Erms_Val,Erms = skeletonForSgd(TRAINING_PHI,W,TEST_PHI,VAL_PHI,ValDataAct,TestDataAct,TrainingTarget)\n",
    "print ('----------Gradient Descent Solution For Human Observed Data Concatenated--------------------')\n",
    "print (\"Accuracy Training   = \" + str(np.around((L_Erm),5)))\n",
    "print (\"Accuracy Validation = \" + str(np.around((L_Erms_Val),5)))\n",
    "print (\"Accuracy Testing    = \" + str(np.around((Erms),5)))\n",
    "L_ERM,Erms_Test,L_Erms_Test = skeletonForSgd(SUB_TRAINING_PHI,SUB_W,SUB_TEST_PHI,SUB_VAL_PHI,ValDataAct,TestDataAct,TrainingTarget)\n",
    "print ('----------Gradient Descent Solution For Human Observed Data Subtracted--------------------')\n",
    "print (\"Accuracy Training   = \" + str(np.around((L_ERM),5)))\n",
    "print (\"Accuracy Validation = \" + str(np.around((Erms_Test),5)))\n",
    "print (\"Accuracy Testing    = \" + str(np.around((L_Erms_Test),5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Gradient Descent Solution For GSC Data Concatenated--------------------\n",
      "Accuracy Training   = 0.79057\n",
      "Accuracy Validation = 0.7307\n",
      "Accuracy Testing    = 0.77775\n",
      "----------Gradient Descent Solution For GSC Data Subtracted--------------------\n",
      "Accuracy Training   = 0.79057\n",
      "Accuracy Validation = 0.59972\n",
      "Accuracy Testing    = 0.8452\n"
     ]
    }
   ],
   "source": [
    "L_Erms_TR,L_Erms_Val,L_Erms_Test = skeletonForSgd(GSC_TRAINING_PHI,GSC_W,GSC_TEST_PHI,GSC_VAL_PHI,GSCVAlDataAct,GSCTestDataAct,GSCTrainingTarget)\n",
    "print ('----------Gradient Descent Solution For GSC Data Concatenated--------------------')\n",
    "print (\"Accuracy Training   = \" + str(np.around((L_Erms_TR),5)))\n",
    "print (\"Accuracy Validation = \" + str(np.around((L_Erm),5)))\n",
    "print (\"Accuracy Testing    = \" + str(np.around((L_ERM),5)))\n",
    "L_Erms_TR,L_Erms_Val,L_Erms_Test = skeletonForSgd(GSC_SUB_TRAINING_PHI,GSC_SUB_W,GSC_SUB_TEST_PHI,GSC_SUB_VAL_PHI,GSCVAlDataAct,GSCTestDataAct,GSCTrainingTarget)\n",
    "print ('----------Gradient Descent Solution For GSC Data Subtracted--------------------')\n",
    "print (\"Accuracy Training   = \" + str(np.around((L_Erms_TR),5)))\n",
    "print (\"Accuracy Validation = \" + str(np.around((Erms),5)))\n",
    "print (\"Accuracy Testing    = \" + str(np.around((Erms_Test),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(TrainingDataForConcatenationHuman,total_human_concat_data,TrainingTarget):\n",
    "    Wlogistic=[]\n",
    "    learningRateLogistic = 0.03\n",
    "    Wlogistic = np.random.random((len(TrainingDataForConcatenationHuman),len(TrainingDataForConcatenationHuman)))\n",
    "    Wlogistic = Wlogistic[1,:]\n",
    "\n",
    "    for i in range(300) :\n",
    "        ysiginput = np.dot(Wlogistic,TrainingDataForConcatenationHuman)\n",
    "        sigoutputa =  1 / (1 + np.exp(-ysiginput))\n",
    "        Gradient =  np.dot(TrainingDataForConcatenationHuman,np.transpose((sigoutputa-ysiginput)))/len(TrainingTarget)\n",
    "        Wlogistic = Wlogistic + learningRateLogistic * Gradient\n",
    "        predicted = np.dot(Wlogistic,TrainingDataForConcatenationHuman)\n",
    "        Accuracy =  1 / (1 + np.exp(-predicted)).mean()*100\n",
    "    return np.around((Accuracy),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.53\n",
      "65.84\n",
      "65.84\n",
      "53.28\n",
      "61.59\n",
      "63.08\n"
     ]
    }
   ],
   "source": [
    "TrainingAccuracy = LogisticRegression(TrainingDataForConcatenationHuman,total_human_concat_data,TrainingTarget)\n",
    "print(TrainingAccuracy)\n",
    "ValidationAccuracy = LogisticRegression((ValDataForHumanConcatenation),total_human_concat_data,(ValDataAct))\n",
    "print(ValidationAccuracy)\n",
    "TestingAccuracy =LogisticRegression((TestDataForConcatenationHuman),total_human_concat_data,(TestDataAct))\n",
    "print(TestingAccuracy)\n",
    "\n",
    "TrainingAccuracy = LogisticRegression(TrainingDataForSubtractionHuman,total_human_subtract_data,TrainingTarget)\n",
    "print(TrainingAccuracy)\n",
    "ValidationAccuracy = LogisticRegression((ValDataForSubtractionHuman),total_human_subtract_data,(ValDataAct))\n",
    "print(ValidationAccuracy)\n",
    "TestingAccuracy =LogisticRegression((TestDataForSubtractionHuman),total_human_subtract_data,(TestDataAct))\n",
    "print(TestingAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.59\n",
      "60.17\n",
      "60.31\n"
     ]
    }
   ],
   "source": [
    "TrainingAccuracy = LogisticRegression(TrainingDataForConcatenationGSC,gsc_total_human_concat_data,GSCTrainingTarget)\n",
    "ValidationAccuracy = LogisticRegression((ValDataForGSCConcatenation),gsc_total_human_concat_data,(GSCVAlDataAct))\n",
    "TestingAccuracy =LogisticRegression((TestDataForGSCConcatenation),gsc_total_human_concat_data,(GSCTestDataAct))\n",
    "\n",
    "TrainingAccuracy = LogisticRegression(TrainingDataForSubtractionGSC,gsc_total_human_subtract_data,GSCTrainingTarget)\n",
    "print(TrainingAccuracy)\n",
    "ValidationAccuracy = LogisticRegression((ValDataForSubtractionGSC),gsc_total_human_subtract_data,(GSCVAlDataAct))\n",
    "print(ValidationAccuracy)\n",
    "TestingAccuracy =LogisticRegression((TestDataForSubtractionGSC),gsc_total_human_subtract_data,(GSCTestDataAct))\n",
    "print(TestingAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "################Neural Networks ################\n",
    "#######half completed- need to find the best suited hyperparameter######\n",
    "def NN(total_human_concat_data,total_target) :\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.callbacks import EarlyStopping, TensorBoard\n",
    "    import numpy\n",
    "    # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    validation_data_split = 0.2\n",
    "    tb_batch_size = 10\n",
    "    early_patience = 100\n",
    "    numpy.random.seed(seed)\n",
    "\n",
    "    # split into input (X) and output (Y) variables\n",
    "    X = np.transpose(total_human_concat_data)\n",
    "    print(len(X))\n",
    "    print(len(X[0]))\n",
    "    Y = total_target\n",
    "\n",
    "    tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "    earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=len(X[0]), init='uniform', activation='relu'))\n",
    "    model.add(Dense(200, init='uniform', activation='relu'))\n",
    "    model.add(Dense(300, init='uniform', activation='relu'))\n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    history=model.fit(X, Y, nb_epoch=10000, batch_size=5,  verbose=2, validation_split=validation_data_split, callbacks = [tensorboard_cb,earlystopping_cb])\n",
    "    # calculate predictions\n",
    "    predictions = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tceve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_dim=18, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\tceve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\tceve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\tceve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\tceve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1265 samples, validate on 317 samples\n",
      "Epoch 1/10000\n",
      " - 1s - loss: 0.3228 - acc: 0.8585 - val_loss: 10.1007 - val_acc: 0.0820\n",
      "Epoch 2/10000\n",
      " - 0s - loss: 0.1496 - acc: 0.9486 - val_loss: 14.2870 - val_acc: 0.0820\n",
      "Epoch 3/10000\n",
      " - 0s - loss: 0.1044 - acc: 0.9684 - val_loss: 14.6381 - val_acc: 0.0820\n",
      "Epoch 4/10000\n",
      " - 0s - loss: 0.0711 - acc: 0.9779 - val_loss: 14.6389 - val_acc: 0.0820\n",
      "Epoch 5/10000\n",
      " - 0s - loss: 0.0505 - acc: 0.9850 - val_loss: 14.6382 - val_acc: 0.0820\n",
      "Epoch 6/10000\n",
      " - 0s - loss: 0.0352 - acc: 0.9913 - val_loss: 14.6396 - val_acc: 0.0820\n",
      "Epoch 7/10000\n",
      " - 0s - loss: 0.0452 - acc: 0.9866 - val_loss: 14.6350 - val_acc: 0.0820\n",
      "Epoch 8/10000\n",
      " - 0s - loss: 0.0358 - acc: 0.9905 - val_loss: 14.5783 - val_acc: 0.0820\n",
      "Epoch 9/10000\n",
      " - 0s - loss: 0.0331 - acc: 0.9897 - val_loss: 14.6370 - val_acc: 0.0820\n",
      "Epoch 10/10000\n",
      " - 0s - loss: 0.0185 - acc: 0.9960 - val_loss: 14.6350 - val_acc: 0.0820\n",
      "Epoch 11/10000\n",
      " - 0s - loss: 0.0072 - acc: 0.9992 - val_loss: 14.6352 - val_acc: 0.0820\n",
      "Epoch 12/10000\n",
      " - 0s - loss: 0.0069 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 13/10000\n",
      " - 0s - loss: 0.0072 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 14/10000\n",
      " - 0s - loss: 0.0072 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 15/10000\n",
      " - 0s - loss: 0.0064 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 16/10000\n",
      " - 0s - loss: 0.0061 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 17/10000\n",
      " - 0s - loss: 0.0059 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 18/10000\n",
      " - 0s - loss: 0.0057 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 19/10000\n",
      " - 0s - loss: 0.0584 - acc: 0.9858 - val_loss: 14.6352 - val_acc: 0.0820\n",
      "Epoch 20/10000\n",
      " - 0s - loss: 0.0223 - acc: 0.9960 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 21/10000\n",
      " - 0s - loss: 0.0068 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 22/10000\n",
      " - 0s - loss: 0.0059 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 23/10000\n",
      " - 0s - loss: 0.0614 - acc: 0.9842 - val_loss: 14.7702 - val_acc: 0.0379\n",
      "Epoch 24/10000\n",
      " - 0s - loss: 0.0186 - acc: 0.9929 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 25/10000\n",
      " - 0s - loss: 0.0058 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 26/10000\n",
      " - 0s - loss: 0.0055 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 27/10000\n",
      " - 0s - loss: 0.0051 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 28/10000\n",
      " - 0s - loss: 0.0055 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 29/10000\n",
      " - 0s - loss: 0.0051 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 30/10000\n",
      " - 0s - loss: 0.0047 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 31/10000\n",
      " - 0s - loss: 0.0403 - acc: 0.9874 - val_loss: 14.6351 - val_acc: 0.0820\n",
      "Epoch 32/10000\n",
      " - 0s - loss: 0.0217 - acc: 0.9937 - val_loss: 14.6350 - val_acc: 0.0820\n",
      "Epoch 33/10000\n",
      " - 0s - loss: 0.0062 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 34/10000\n",
      " - 0s - loss: 0.0059 - acc: 0.9992 - val_loss: 14.6350 - val_acc: 0.0820\n",
      "Epoch 35/10000\n",
      " - 0s - loss: 0.0049 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 36/10000\n",
      " - 0s - loss: 0.0047 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 37/10000\n",
      " - 0s - loss: 0.0053 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 38/10000\n",
      " - 0s - loss: 0.0122 - acc: 0.9968 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 39/10000\n",
      " - 0s - loss: 0.0054 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 40/10000\n",
      " - 0s - loss: 0.0053 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 41/10000\n",
      " - 0s - loss: 0.0212 - acc: 0.9937 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 42/10000\n",
      " - 0s - loss: 0.0579 - acc: 0.9818 - val_loss: 14.6351 - val_acc: 0.0820\n",
      "Epoch 43/10000\n",
      " - 0s - loss: 0.0045 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 44/10000\n",
      " - 0s - loss: 0.0073 - acc: 0.9992 - val_loss: 14.6350 - val_acc: 0.0820\n",
      "Epoch 45/10000\n",
      " - 0s - loss: 0.0051 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 46/10000\n",
      " - 0s - loss: 0.0080 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 47/10000\n",
      " - 0s - loss: 0.0053 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 48/10000\n",
      " - 0s - loss: 0.0049 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 49/10000\n",
      " - 0s - loss: 0.0048 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 50/10000\n",
      " - 0s - loss: 0.0322 - acc: 0.9905 - val_loss: 14.6364 - val_acc: 0.0820\n",
      "Epoch 51/10000\n",
      " - 0s - loss: 0.0057 - acc: 0.9984 - val_loss: 14.6352 - val_acc: 0.0820\n",
      "Epoch 52/10000\n",
      " - 0s - loss: 0.0037 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 53/10000\n",
      " - 0s - loss: 0.0043 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 54/10000\n",
      " - 0s - loss: 0.0040 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 55/10000\n",
      " - 0s - loss: 0.0040 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 56/10000\n",
      " - 0s - loss: 0.0039 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 57/10000\n",
      " - 0s - loss: 0.0039 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 58/10000\n",
      " - 0s - loss: 0.0039 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 59/10000\n",
      " - 0s - loss: 0.0034 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 60/10000\n",
      " - 0s - loss: 0.0030 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 61/10000\n",
      " - 0s - loss: 0.0042 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 62/10000\n",
      " - 0s - loss: 0.0042 - acc: 0.9992 - val_loss: 14.6351 - val_acc: 0.0820\n",
      "Epoch 63/10000\n",
      " - 0s - loss: 0.0286 - acc: 0.9921 - val_loss: 14.6353 - val_acc: 0.0820\n",
      "Epoch 64/10000\n",
      " - 0s - loss: 0.0112 - acc: 0.9976 - val_loss: 14.6351 - val_acc: 0.0820\n",
      "Epoch 65/10000\n",
      " - 0s - loss: 0.0037 - acc: 0.9992 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 66/10000\n",
      " - 0s - loss: 0.0024 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 67/10000\n",
      " - 0s - loss: 0.0036 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 68/10000\n",
      " - 0s - loss: 0.0037 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 69/10000\n",
      " - 0s - loss: 0.0044 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 70/10000\n",
      " - 0s - loss: 0.0032 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 71/10000\n",
      " - 0s - loss: 0.0030 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 72/10000\n",
      " - 0s - loss: 0.0025 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 73/10000\n",
      " - 0s - loss: 0.0036 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 74/10000\n",
      " - 0s - loss: 0.0036 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 75/10000\n",
      " - 0s - loss: 0.0022 - acc: 0.9992 - val_loss: 14.6354 - val_acc: 0.0820\n",
      "Epoch 76/10000\n",
      " - 0s - loss: 0.0022 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 77/10000\n",
      " - 0s - loss: 0.0024 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 78/10000\n",
      " - 0s - loss: 0.0430 - acc: 0.9897 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 79/10000\n",
      " - 0s - loss: 0.0432 - acc: 0.9921 - val_loss: 14.6384 - val_acc: 0.0820\n",
      "Epoch 80/10000\n",
      " - 0s - loss: 0.0091 - acc: 0.9984 - val_loss: 14.6351 - val_acc: 0.0820\n",
      "Epoch 81/10000\n",
      " - 0s - loss: 0.0127 - acc: 0.9953 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 82/10000\n",
      " - 0s - loss: 0.0064 - acc: 0.9976 - val_loss: 14.6349 - val_acc: 0.0820\n",
      "Epoch 83/10000\n",
      " - 0s - loss: 0.0044 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 84/10000\n",
      " - 0s - loss: 0.0034 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 85/10000\n",
      " - 0s - loss: 0.0026 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 86/10000\n",
      " - 0s - loss: 0.0020 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 87/10000\n",
      " - 0s - loss: 0.0052 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 88/10000\n",
      " - 0s - loss: 0.0031 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 89/10000\n",
      " - 0s - loss: 0.0025 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 90/10000\n",
      " - 0s - loss: 9.4386e-04 - acc: 1.0000 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 91/10000\n",
      " - 0s - loss: 0.0040 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 92/10000\n",
      " - 0s - loss: 0.0019 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 93/10000\n",
      " - 0s - loss: 0.0045 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/10000\n",
      " - 0s - loss: 0.0017 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 95/10000\n",
      " - 0s - loss: 8.2900e-04 - acc: 1.0000 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 96/10000\n",
      " - 0s - loss: 0.0024 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 97/10000\n",
      " - 0s - loss: 0.0017 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 98/10000\n",
      " - 0s - loss: 0.0015 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 99/10000\n",
      " - 0s - loss: 0.0011 - acc: 0.9992 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 100/10000\n",
      " - 0s - loss: 0.0038 - acc: 0.9984 - val_loss: 14.6348 - val_acc: 0.0820\n",
      "Epoch 101/10000\n",
      " - 0s - loss: 0.0268 - acc: 0.9913 - val_loss: 14.6327 - val_acc: 0.0820\n",
      "Epoch 00101: early stopping\n",
      "1582\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tceve\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, input_dim=9, activation=\"relu\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1265 samples, validate on 317 samples\n",
      "Epoch 1/10000\n",
      " - 1s - loss: 0.4549 - acc: 0.7723 - val_loss: 2.3898 - val_acc: 0.2050\n",
      "Epoch 2/10000\n",
      " - 0s - loss: 0.3512 - acc: 0.8427 - val_loss: 2.9461 - val_acc: 0.1640\n",
      "Epoch 3/10000\n",
      " - 0s - loss: 0.3240 - acc: 0.8569 - val_loss: 2.8295 - val_acc: 0.1262\n",
      "Epoch 4/10000\n",
      " - 0s - loss: 0.2894 - acc: 0.8688 - val_loss: 4.8912 - val_acc: 0.1451\n",
      "Epoch 5/10000\n",
      " - 0s - loss: 0.2662 - acc: 0.8791 - val_loss: 6.7130 - val_acc: 0.0852\n",
      "Epoch 6/10000\n",
      " - 0s - loss: 0.2698 - acc: 0.8791 - val_loss: 2.9655 - val_acc: 0.1735\n",
      "Epoch 7/10000\n",
      " - 0s - loss: 0.2454 - acc: 0.8972 - val_loss: 5.6004 - val_acc: 0.1546\n",
      "Epoch 8/10000\n",
      " - 0s - loss: 0.2394 - acc: 0.8957 - val_loss: 4.5496 - val_acc: 0.1388\n",
      "Epoch 9/10000\n",
      " - 0s - loss: 0.2197 - acc: 0.9028 - val_loss: 5.4895 - val_acc: 0.1609\n",
      "Epoch 10/10000\n",
      " - 0s - loss: 0.2124 - acc: 0.9107 - val_loss: 7.2415 - val_acc: 0.1356\n",
      "Epoch 11/10000\n",
      " - 0s - loss: 0.2009 - acc: 0.9130 - val_loss: 7.6577 - val_acc: 0.1230\n",
      "Epoch 12/10000\n",
      " - 0s - loss: 0.1919 - acc: 0.9075 - val_loss: 6.9034 - val_acc: 0.1514\n",
      "Epoch 13/10000\n",
      " - 0s - loss: 0.1836 - acc: 0.9209 - val_loss: 9.6706 - val_acc: 0.0883\n",
      "Epoch 14/10000\n",
      " - 0s - loss: 0.1648 - acc: 0.9296 - val_loss: 9.8749 - val_acc: 0.1073\n",
      "Epoch 15/10000\n",
      " - 0s - loss: 0.1620 - acc: 0.9312 - val_loss: 10.3477 - val_acc: 0.0852\n",
      "Epoch 16/10000\n",
      " - 0s - loss: 0.1510 - acc: 0.9304 - val_loss: 10.8746 - val_acc: 0.1009\n",
      "Epoch 17/10000\n",
      " - 0s - loss: 0.1464 - acc: 0.9407 - val_loss: 10.9588 - val_acc: 0.1230\n",
      "Epoch 18/10000\n",
      " - 0s - loss: 0.1506 - acc: 0.9368 - val_loss: 10.6179 - val_acc: 0.0978\n",
      "Epoch 19/10000\n",
      " - 0s - loss: 0.1340 - acc: 0.9439 - val_loss: 10.1432 - val_acc: 0.0789\n",
      "Epoch 20/10000\n",
      " - 0s - loss: 0.1190 - acc: 0.9494 - val_loss: 10.1259 - val_acc: 0.1009\n",
      "Epoch 21/10000\n",
      " - 0s - loss: 0.1252 - acc: 0.9431 - val_loss: 11.3118 - val_acc: 0.0915\n",
      "Epoch 22/10000\n",
      " - 0s - loss: 0.1192 - acc: 0.9510 - val_loss: 11.4465 - val_acc: 0.0694\n",
      "Epoch 23/10000\n",
      " - 0s - loss: 0.1211 - acc: 0.9518 - val_loss: 11.0019 - val_acc: 0.0789\n",
      "Epoch 24/10000\n",
      " - 0s - loss: 0.0907 - acc: 0.9613 - val_loss: 12.0991 - val_acc: 0.0726\n",
      "Epoch 25/10000\n",
      " - 0s - loss: 0.0815 - acc: 0.9636 - val_loss: 11.8896 - val_acc: 0.0820\n",
      "Epoch 26/10000\n",
      " - 0s - loss: 0.0811 - acc: 0.9621 - val_loss: 12.3603 - val_acc: 0.0915\n",
      "Epoch 27/10000\n",
      " - 0s - loss: 0.0943 - acc: 0.9605 - val_loss: 11.6927 - val_acc: 0.1041\n",
      "Epoch 28/10000\n",
      " - 0s - loss: 0.0979 - acc: 0.9668 - val_loss: 10.7318 - val_acc: 0.1073\n",
      "Epoch 29/10000\n",
      " - 0s - loss: 0.0787 - acc: 0.9700 - val_loss: 11.0583 - val_acc: 0.0946\n",
      "Epoch 30/10000\n",
      " - 0s - loss: 0.0617 - acc: 0.9771 - val_loss: 11.1985 - val_acc: 0.1451\n",
      "Epoch 31/10000\n",
      " - 0s - loss: 0.0585 - acc: 0.9787 - val_loss: 12.3286 - val_acc: 0.0757\n",
      "Epoch 32/10000\n",
      " - 0s - loss: 0.0597 - acc: 0.9747 - val_loss: 11.8080 - val_acc: 0.0852\n",
      "Epoch 33/10000\n",
      " - 0s - loss: 0.0542 - acc: 0.9787 - val_loss: 12.4592 - val_acc: 0.0757\n",
      "Epoch 34/10000\n",
      " - 0s - loss: 0.0950 - acc: 0.9684 - val_loss: 12.4821 - val_acc: 0.0852\n",
      "Epoch 35/10000\n",
      " - 0s - loss: 0.0620 - acc: 0.9763 - val_loss: 12.2731 - val_acc: 0.0946\n",
      "Epoch 36/10000\n",
      " - 0s - loss: 0.0437 - acc: 0.9850 - val_loss: 12.1661 - val_acc: 0.0978\n",
      "Epoch 37/10000\n",
      " - 0s - loss: 0.0334 - acc: 0.9874 - val_loss: 12.8068 - val_acc: 0.0852\n",
      "Epoch 38/10000\n",
      " - 0s - loss: 0.0314 - acc: 0.9889 - val_loss: 12.7375 - val_acc: 0.0883\n",
      "Epoch 39/10000\n",
      " - 0s - loss: 0.0270 - acc: 0.9905 - val_loss: 12.7863 - val_acc: 0.0883\n",
      "Epoch 40/10000\n",
      " - 0s - loss: 0.1191 - acc: 0.9644 - val_loss: 12.7827 - val_acc: 0.0820\n",
      "Epoch 41/10000\n",
      " - 0s - loss: 0.0640 - acc: 0.9755 - val_loss: 12.9478 - val_acc: 0.0757\n",
      "Epoch 42/10000\n",
      " - 0s - loss: 0.0606 - acc: 0.9763 - val_loss: 11.7173 - val_acc: 0.1073\n",
      "Epoch 43/10000\n",
      " - 0s - loss: 0.0325 - acc: 0.9881 - val_loss: 12.7558 - val_acc: 0.0883\n",
      "Epoch 44/10000\n",
      " - 0s - loss: 0.0245 - acc: 0.9913 - val_loss: 12.7962 - val_acc: 0.0883\n",
      "Epoch 45/10000\n",
      " - 0s - loss: 0.0401 - acc: 0.9858 - val_loss: 11.6162 - val_acc: 0.1483\n",
      "Epoch 46/10000\n",
      " - 0s - loss: 0.0472 - acc: 0.9818 - val_loss: 11.8480 - val_acc: 0.1230\n",
      "Epoch 47/10000\n",
      " - 0s - loss: 0.0549 - acc: 0.9763 - val_loss: 12.7386 - val_acc: 0.0883\n",
      "Epoch 48/10000\n",
      " - 0s - loss: 0.0318 - acc: 0.9858 - val_loss: 12.4201 - val_acc: 0.0915\n",
      "Epoch 49/10000\n",
      " - 0s - loss: 0.0324 - acc: 0.9874 - val_loss: 12.0432 - val_acc: 0.1136\n",
      "Epoch 50/10000\n",
      " - 0s - loss: 0.0266 - acc: 0.9874 - val_loss: 12.0543 - val_acc: 0.1230\n",
      "Epoch 51/10000\n",
      " - 0s - loss: 0.0386 - acc: 0.9881 - val_loss: 12.1015 - val_acc: 0.1136\n",
      "Epoch 52/10000\n",
      " - 0s - loss: 0.0188 - acc: 0.9897 - val_loss: 12.5161 - val_acc: 0.1073\n",
      "Epoch 53/10000\n",
      " - 0s - loss: 0.0168 - acc: 0.9921 - val_loss: 12.8667 - val_acc: 0.0915\n",
      "Epoch 54/10000\n",
      " - 0s - loss: 0.0138 - acc: 0.9937 - val_loss: 12.9045 - val_acc: 0.1009\n",
      "Epoch 55/10000\n",
      " - 0s - loss: 0.0113 - acc: 0.9953 - val_loss: 12.8961 - val_acc: 0.0978\n",
      "Epoch 56/10000\n",
      " - 0s - loss: 0.0583 - acc: 0.9842 - val_loss: 12.4083 - val_acc: 0.0978\n",
      "Epoch 57/10000\n",
      " - 0s - loss: 0.0782 - acc: 0.9747 - val_loss: 11.3099 - val_acc: 0.1735\n",
      "Epoch 58/10000\n",
      " - 0s - loss: 0.0401 - acc: 0.9866 - val_loss: 12.0815 - val_acc: 0.1104\n",
      "Epoch 59/10000\n",
      " - 0s - loss: 0.0328 - acc: 0.9889 - val_loss: 13.0479 - val_acc: 0.0852\n",
      "Epoch 60/10000\n",
      " - 0s - loss: 0.0495 - acc: 0.9810 - val_loss: 12.8638 - val_acc: 0.0852\n",
      "Epoch 61/10000\n",
      " - 0s - loss: 0.0331 - acc: 0.9874 - val_loss: 12.5653 - val_acc: 0.0915\n",
      "Epoch 62/10000\n",
      " - 0s - loss: 0.0516 - acc: 0.9826 - val_loss: 12.5212 - val_acc: 0.1073\n",
      "Epoch 63/10000\n",
      " - 0s - loss: 0.0250 - acc: 0.9874 - val_loss: 12.5357 - val_acc: 0.1073\n",
      "Epoch 64/10000\n",
      " - 0s - loss: 0.0159 - acc: 0.9921 - val_loss: 12.9480 - val_acc: 0.0883\n",
      "Epoch 65/10000\n",
      " - 0s - loss: 0.0151 - acc: 0.9945 - val_loss: 13.1012 - val_acc: 0.0852\n",
      "Epoch 66/10000\n",
      " - 0s - loss: 0.0134 - acc: 0.9953 - val_loss: 13.0184 - val_acc: 0.0915\n",
      "Epoch 67/10000\n",
      " - 0s - loss: 0.0111 - acc: 0.9945 - val_loss: 13.1016 - val_acc: 0.0915\n",
      "Epoch 68/10000\n",
      " - 0s - loss: 0.0096 - acc: 0.9968 - val_loss: 13.1044 - val_acc: 0.0946\n",
      "Epoch 69/10000\n",
      " - 0s - loss: 0.0081 - acc: 0.9968 - val_loss: 13.2363 - val_acc: 0.0915\n",
      "Epoch 70/10000\n",
      " - 0s - loss: 0.0121 - acc: 0.9945 - val_loss: 13.2356 - val_acc: 0.0915\n",
      "Epoch 71/10000\n",
      " - 0s - loss: 0.0236 - acc: 0.9921 - val_loss: 12.2946 - val_acc: 0.1483\n",
      "Epoch 72/10000\n",
      " - 0s - loss: 0.0501 - acc: 0.9858 - val_loss: 11.7042 - val_acc: 0.1451\n",
      "Epoch 73/10000\n",
      " - 0s - loss: 0.0683 - acc: 0.9739 - val_loss: 11.4254 - val_acc: 0.1420\n",
      "Epoch 74/10000\n",
      " - 0s - loss: 0.0333 - acc: 0.9889 - val_loss: 11.8817 - val_acc: 0.1104\n",
      "Epoch 75/10000\n",
      " - 0s - loss: 0.0162 - acc: 0.9937 - val_loss: 12.2798 - val_acc: 0.1262\n",
      "Epoch 76/10000\n",
      " - 0s - loss: 0.0182 - acc: 0.9905 - val_loss: 12.5095 - val_acc: 0.1073\n",
      "Epoch 77/10000\n",
      " - 0s - loss: 0.0298 - acc: 0.9874 - val_loss: 12.4721 - val_acc: 0.1104\n",
      "Epoch 78/10000\n",
      " - 0s - loss: 0.0185 - acc: 0.9921 - val_loss: 12.6527 - val_acc: 0.1167\n",
      "Epoch 79/10000\n",
      " - 0s - loss: 0.0081 - acc: 0.9960 - val_loss: 12.7766 - val_acc: 0.1136\n",
      "Epoch 80/10000\n",
      " - 0s - loss: 0.0092 - acc: 0.9953 - val_loss: 12.7317 - val_acc: 0.1136\n",
      "Epoch 81/10000\n",
      " - 0s - loss: 0.0060 - acc: 0.9968 - val_loss: 12.8707 - val_acc: 0.1136\n",
      "Epoch 82/10000\n",
      " - 0s - loss: 0.0064 - acc: 0.9976 - val_loss: 12.9869 - val_acc: 0.1136\n",
      "Epoch 83/10000\n",
      " - 0s - loss: 0.0102 - acc: 0.9968 - val_loss: 12.8467 - val_acc: 0.1167\n",
      "Epoch 84/10000\n",
      " - 0s - loss: 0.0126 - acc: 0.9937 - val_loss: 12.8426 - val_acc: 0.1199\n",
      "Epoch 85/10000\n",
      " - 0s - loss: 0.0296 - acc: 0.9874 - val_loss: 12.7334 - val_acc: 0.1136\n",
      "Epoch 86/10000\n",
      " - 0s - loss: 0.1040 - acc: 0.9700 - val_loss: 10.2164 - val_acc: 0.1767\n",
      "Epoch 87/10000\n",
      " - 0s - loss: 0.0341 - acc: 0.9858 - val_loss: 11.5533 - val_acc: 0.1388\n",
      "Epoch 88/10000\n",
      " - 0s - loss: 0.0194 - acc: 0.9921 - val_loss: 12.2200 - val_acc: 0.1167\n",
      "Epoch 89/10000\n",
      " - 0s - loss: 0.0345 - acc: 0.9850 - val_loss: 12.0308 - val_acc: 0.1167\n",
      "Epoch 90/10000\n",
      " - 0s - loss: 0.0267 - acc: 0.9881 - val_loss: 12.0082 - val_acc: 0.1041\n",
      "Epoch 91/10000\n",
      " - 0s - loss: 0.0115 - acc: 0.9953 - val_loss: 12.1832 - val_acc: 0.1136\n",
      "Epoch 92/10000\n",
      " - 0s - loss: 0.0072 - acc: 0.9960 - val_loss: 12.2710 - val_acc: 0.1199\n",
      "Epoch 93/10000\n",
      " - 0s - loss: 0.0067 - acc: 0.9968 - val_loss: 12.5345 - val_acc: 0.1041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/10000\n",
      " - 0s - loss: 0.0080 - acc: 0.9960 - val_loss: 12.4019 - val_acc: 0.1136\n",
      "Epoch 95/10000\n",
      " - 0s - loss: 0.0065 - acc: 0.9945 - val_loss: 12.5277 - val_acc: 0.1136\n",
      "Epoch 96/10000\n",
      " - 0s - loss: 0.0065 - acc: 0.9953 - val_loss: 12.6241 - val_acc: 0.1104\n",
      "Epoch 97/10000\n",
      " - 0s - loss: 0.0059 - acc: 0.9968 - val_loss: 12.7166 - val_acc: 0.1073\n",
      "Epoch 98/10000\n",
      " - 0s - loss: 0.0059 - acc: 0.9968 - val_loss: 12.5786 - val_acc: 0.1199\n",
      "Epoch 99/10000\n",
      " - 0s - loss: 0.0057 - acc: 0.9960 - val_loss: 12.5850 - val_acc: 0.1167\n",
      "Epoch 100/10000\n",
      " - 0s - loss: 0.0195 - acc: 0.9937 - val_loss: 12.3431 - val_acc: 0.1009\n",
      "Epoch 101/10000\n",
      " - 0s - loss: 0.0088 - acc: 0.9968 - val_loss: 12.5329 - val_acc: 0.0946\n",
      "Epoch 00101: early stopping\n"
     ]
    }
   ],
   "source": [
    "NN(total_human_concat_data,total_target)\n",
    "NN(total_human_subtract_data,total_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
